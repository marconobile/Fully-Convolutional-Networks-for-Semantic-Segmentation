{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Let's load a single picture from the train_set\n",
    "import imageio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Resize\n",
    "from PIL import Image \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-167a45b4bf55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# ###################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mjust_to_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjust_to_show\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemplate_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/DL_pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DL_pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    #########################################\n",
    "#     root ='./train_templates/'\n",
    "#     cwd = os.getcwd()\n",
    "\n",
    "    # Data augmentation is a good practice for the train set\n",
    "    # Here, we randomly crop the image to 224x224 and\n",
    "    # randomly flip it horizontally.\n",
    "\n",
    "data_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                       transforms.RandomGrayscale(p=0.1),\n",
    "#                                       transforms.RandomHorizontalFlip(),\n",
    "#                                       transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "#                                       transforms.RandomAffine(degrees=45, translate=(0,1), scale=(.1,1)),\n",
    "#                                       transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3),\n",
    "#                                       transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0),\n",
    "#                                                                   ratio=(0.75, 1.3333333333333333),\n",
    "#                                                                   interpolation=2),\n",
    "                                      \n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                           std= [0.229, 0.224, 0.225])])\n",
    "\n",
    "template_data = datasets.ImageFolder(root = '/Users/marconobile/Desktop/CVPR_PRJ/train_templates',\n",
    "                                     data_transforms=data_transforms)\n",
    "\n",
    "## cast data to list and shuffle\n",
    "#data = []\n",
    "#for el in template_data:\n",
    " #   data.append(el)\n",
    "\n",
    "#shuffle(data)\n",
    "\n",
    "#train_dataset = data[0:int(0.85*len(data))]\n",
    "#val_dataset = data[int(0.85 * len(data)):]\n",
    "\n",
    "#print(f'Len of original dataset {len(data)}, len of training data {len(train_dataset)}, len of val data {len(val_dataset)}')\n",
    "\n",
    "#train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle= True)\n",
    "#val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 32, shuffle= True)\n",
    "\n",
    "# ###################################################################\n",
    "just_to_show = torch.utils.data.DataLoader(template_data, batch_size = 5, shuffle= True)\n",
    "inputs, classes = next(iter(just_to_show))\n",
    "class_names = template_data.classes\n",
    "out = utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])\n",
    "    \n",
    "    \n",
    "\n",
    "# # ##########################Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/marconobile/Desktop/CVPR_PRJ/FullIJCNN2013/00000.ppm'\n",
    "# image = imageio.imread(path)\n",
    "# plt.imshow(image)\n",
    "# # 00000.ppm;774;411;815;446;11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_pil = ToPILImage(mode='RGB')\n",
    "# resize_pic = Resize(512, interpolation=2)\n",
    "# to_tens = ToTensor()\n",
    "# # normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# image = plt.imread(path)\n",
    "# image = to_pil(image)\n",
    "# image = resize_pic(image)\n",
    "# plt.imshow(image)\n",
    "# # image = to_tens(image)\n",
    "# # image = normalize(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import + refactor dataset to grab labels from g.txt file s.t we can slice the output of the net correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "# from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_bool\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "class Target_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, picture_dict, labels_dict):  # LIST OF PICTURES TAKE FROM THE FOLDER\n",
    "        self.picture_dict = picture_dict\n",
    "        self.labels_dict = labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_dict.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        name = list(self.labels_dict.keys())[idx]\n",
    "        h, w = (self.picture_dict[name].shape[1], self.picture_dict[name].shape[2])\n",
    "        h = h//8\n",
    "        w = w // 8\n",
    "        # print('original h,w',h,w)\n",
    "\n",
    "        empty_labels = torch.zeros(([43, 800, 1360]))\n",
    "        label_bkg = torch.ones(([1, 800, 1360]))\n",
    "        for patch in self.labels_dict[name]:\n",
    "            if int(patch[4]) != 43:\n",
    "                empty_labels[int(patch[4]), int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 1\n",
    "                label_bkg[0, int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 0\n",
    "\n",
    "        target = torch.cat((empty_labels, label_bkg), dim=0)\n",
    "        # 44 800,1360\n",
    "\n",
    "        for i in range(target.shape[0]):\n",
    "            if i == 0:\n",
    "                resized = img_as_bool(resize(target[i], (h, w)))\n",
    "                resized_1 = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "                # 1 25 42\n",
    "            else:\n",
    "                resized = img_as_bool(resize(target[i], (h, w)))\n",
    "                asd = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "                resized_1 = torch.cat((resized_1, asd), dim=0)\n",
    "\n",
    "        target = resized_1\n",
    "        # print('target.shape', target.shape)\n",
    "        return self.picture_dict[name], target\n",
    "\n",
    "def get_labels_dict():\n",
    "    obs_data = {}\n",
    "    with open(\"./gt.txt\", \"r\") as file:\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            LINE = line.split(';')\n",
    "            if LINE[0] not in obs_data.keys():\n",
    "                obs_data[LINE[0]] = [[LINE[1],LINE[2],LINE[3],LINE[4],LINE[5].rstrip('\\n')]]\n",
    "            else:\n",
    "                obs_data[LINE[0]].append([LINE[1],LINE[2],LINE[3],LINE[4],LINE[5].rstrip('\\n')])\n",
    "    return obs_data\n",
    "\n",
    "\n",
    "def get_picture_dict(path):\n",
    "    to_pil = transforms.ToPILImage(mode='RGB')\n",
    "    resize_pic = transforms.Resize(512, interpolation=2)\n",
    "    to_tens = transforms.ToTensor()\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    x_dict = {}\n",
    "    for _, _, all_pics in os.walk(path, topdown=True):\n",
    "        for pic in all_pics:\n",
    "            if pic[0] != '.':\n",
    "                image = plt.imread(path + pic)\n",
    "                image = to_pil(image)\n",
    "                image = resize_pic(image)\n",
    "                image = to_tens(image)\n",
    "                x_dict[pic] = normalize(image)\n",
    "                # x_dict[pic] = image\n",
    "\n",
    "    return x_dict\n",
    "\n",
    "\n",
    "def get_prediction_for_single_pic(path, model):\n",
    "    to_pil = transforms.ToPILImage(mode='RGB')\n",
    "    # resize_pic = transforms.Resize(400, interpolation=2)\n",
    "    to_tens = transforms.ToTensor()\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    image = plt.imread(path)\n",
    "    image = to_pil(image)\n",
    "    # image = resize_pic(image)\n",
    "    image = to_tens(image)\n",
    "    image = normalize(image) # (C x H x W) 3D\n",
    "    print('INPUT dim', image.shape)\n",
    "    ## add the batch dimension\n",
    "    prediction = model(torch.unsqueeze(image, dim=0))  # [1 ,RGB, w, h] to 4D\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to see the ones in the HD version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_labels_dict()) # '00000.ppm': [['774', '411', '815', '446', '11']]\n",
    "path = './FullIJCNN2013/'\n",
    "dataset_targets = Target_dataset(get_picture_dict(path), get_labels_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset_targets[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28,\n",
       "        28, 29, 29, 29, 29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(y[13,:,:], as_tuple = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 24, 25, 23, 24, 25, 26, 89, 90, 91, 92, 23, 24, 25, 26, 89, 90, 91,\n",
       "        92, 89, 90, 91, 92])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(y[13,:,:], as_tuple = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['1125', '338', '1168', '378', '13']\n",
    "# ['288', '328', '334', '368', '13']\n",
    "# ['148', '446', '196', '499', '38']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False],\n",
       "        [False,  True,  True,  True, False, False],\n",
       "        [False,  True,  True,  True,  True, False],\n",
       "        [False,  True,  True,  True,  True, False],\n",
       "        [False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False, False, False, False, False],\n",
       "        [False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[13,22:28,88:93 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IOU(path, model, labels_dict):\n",
    "\n",
    "    # step 1: get the prediction for that given image:\n",
    "    prediction = get_prediction_for_single_pic(path, model)  # which is 4D: [1 ,44, h, w]\n",
    "    # get it's 3D version [44,w,h]\n",
    "    prediction = torch.squeeze(prediction, dim=0) # [44, h, w]\n",
    "\n",
    "    # step 2: get the key to retrieve the true bounding boxes in labels_dict:\n",
    "    name = path[path.rfind('/') + 1:]\n",
    "\n",
    "    # step 3\n",
    "    # get in dict: then for each box coords in that picture, we grab area\n",
    "    traffic_signs_in_pic = []\n",
    "    for box_coords in labels_dict[name]:\n",
    "        traffic_signs_in_pic.append(box_coords[-1])\n",
    "        print('original' , box_coords)\n",
    "        leftCol = int(box_coords[0]) // 8\n",
    "        topRow = int(box_coords[1]) // 8\n",
    "        rightCol = int(box_coords[2]) // 8\n",
    "        bottomRow = int(box_coords[3]) // 8\n",
    "        ts = int(box_coords[4])\n",
    "        print('1/8', leftCol,topRow,rightCol,bottomRow)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1125//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140, 42, 146, 47]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['1125', '338', '1168', '378']\n",
    "new_coords1 = [int(el)//8 for el in a]\n",
    "new_coords1 # but actually theese are [42:47, 139:144]  # -1, -2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 41, 41, 46]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ['288', '328', '334', '368']\n",
    "new_coords2 = [int(el)//8 for el in b]\n",
    "new_coords2\n",
    "\n",
    "# [41:46,35:39] # target[13, 41:46, 36:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 55, 24, 62]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = ['148', '446', '196', '499']\n",
    "new_coords3 = [int(el)//8 for el in c]\n",
    "new_coords3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # leftCol #topRow #rightCol #bottomRow #ClassID\n",
    "\n",
    "# [42:47,140:146] \n",
    "# [41:46,36:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 146 42 47\n"
     ]
    }
   ],
   "source": [
    "print(new_coords1[1],new_coords1[3],new_coords1[0],new_coords1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[13,25:30, 22:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[13,25:30, 22:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " empty_labels = torch.zeros(([43, 800, 1360]))\n",
    "        label_bkg = torch.ones(([1, 800, 1360]))\n",
    "        for patch in labels_dict[name]:\n",
    "            if int(patch[4]) != 43:\n",
    "                empty_labels[int(patch[4]), int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 1\n",
    "                label_bkg[0, int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 0\n",
    "\n",
    "        target = torch.cat((empty_labels, label_bkg), dim=0)\n",
    "        # 44 800,1360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each hd prediction,\n",
    "\n",
    "take the HD boudning box\n",
    "recompute the bounding box cords\n",
    "fix em\n",
    "take the target of that picture\n",
    "take the sum over each box\n",
    "do the same for the prediction \n",
    "compute the ratio\n",
    "sum the ration and keep tracks of the number of els in the sum for the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-792335a2ee00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcurrent_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'm'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/marconobile/Desktop/CVPR_PRJ/FullIJCNN2013/'\n",
    "# def get_IOU(path, model, labels_dict):\n",
    "    \n",
    "#     IOU = 0\n",
    "#     N = 0\n",
    "#     for root, dirs, files in os.walk(path, topdown=True):\n",
    "#         for name in files:\n",
    "#             current_file = str(os.path.join(root, name))\n",
    "#             if (current_file[-1] == 'm') and (current_file[current_file.rfind('/') + 1:] in list(labels_dict.keys())):\n",
    "#                 prediction = get_prediction_for_single_pic(current_file, model)\n",
    "#                 prediction = torch.squeeze(prediction, dim=0) # [44, h, w]\n",
    "#                 prediction_ = torch.nn.functional.softmax(prediction, dim=0)\n",
    "# #                 prediction_1 = torch.max(prediction_[:-1], dim = 0, keepdim=True)\n",
    "                \n",
    "#                 name = current_file[current_file.rfind('/') + 1:]\n",
    "#                 ###############################\n",
    "#                 empty_labels = torch.zeros(([43, 800, 1360]))\n",
    "#                 label_bkg = torch.ones(([1, 800, 1360]))\n",
    "#                 for patch in labels_dict[name]:\n",
    "#                     if int(patch[4]) != 43:\n",
    "#                         empty_labels[int(patch[4]), int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 1\n",
    "#                         label_bkg[0, int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 0\n",
    "#                 target = torch.cat((empty_labels, label_bkg), dim=0)\n",
    "#                 for i in range(target.shape[0]):\n",
    "#                     if i == 0:\n",
    "#                         resized = img_as_bool(resize(target[i], (prediction_.shape[1], prediction_.shape[2])))\n",
    "#                         resized_1 = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "#                         # 1 25 42\n",
    "#                     else:\n",
    "#                         resized = img_as_bool(resize(target[i],(prediction_.shape[1], prediction_.shape[2])))\n",
    "#                         asd = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "#                         resized_1 = torch.cat((resized_1, asd), dim=0)\n",
    "\n",
    "#                 target = resized_1 #target[13, 42:47, 139:144]\n",
    "\n",
    "#                 ###############################\n",
    "#                 traffic_signs_in_pic = []\n",
    "#                 for box_coords in labels_dict[name]:\n",
    "#                     traffic_signs_in_pic.append(box_coords[-1])\n",
    "#                     new_coords = [int(el)//8 for el in box_coords[:-1]]\n",
    "#                     new_coords1[1] = new_coords1[1] - 2 \n",
    "#                     new_coords1[3] = new_coords1[3] + 2\n",
    "#                     new_coords1[0] = new_coords1[0] - 2\n",
    "#                     new_coords1[2] = new_coords1[2] + 2\n",
    "                    \n",
    "#                     area_target = torch.sum(target[box_coords[-1], new_coords1[1]:new_coords1[3],\n",
    "#                                                    new_coords1[0]:new_coords1[2]]).item()\n",
    "                    \n",
    "                    \n",
    "#                     our_value = torch.sum(torch.round(prediction_[, new_coords1[1]:new_coords1[3],\n",
    "#                                    new_coords1[0]:new_coords1[2]]).item()\n",
    "                        \n",
    "#                     N+=1\n",
    "#                     IOU+= (our_value/area_target)\n",
    "\n",
    "#     return IOU/N\n",
    "                    \n",
    "# # print(new_coords1[1],new_coords1[3],new_coords1[0],new_coords1[2])\n",
    "                \n",
    "#                 ###############################\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# labels_dict =  get_labels_dict()\n",
    "# path = '/Users/marconobile/Desktop/CVPR_PRJ/FullIJCNN2013/00824'\n",
    "# ###############################\n",
    "# empty_labels = torch.zeros(([43, 800, 1360]))\n",
    "# label_bkg = torch.ones(([1, 800, 1360]))\n",
    "# for patch in labels_dict[name]:\n",
    "#     if int(patch[4]) != 43:\n",
    "#         empty_labels[int(patch[4]), int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 1\n",
    "#         label_bkg[0, int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 0\n",
    "# target = torch.cat((empty_labels, label_bkg), dim=0)\n",
    "# print(target[15 ,410:421,252:267])\n",
    "# for i in range(target.shape[0]):\n",
    "#     if i == 0:\n",
    "#         resized = img_as_bool(resize(target[i], (100,168)))\n",
    "#         resized_1 = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "#         # 1 25 42\n",
    "#     else:\n",
    "#         resized = img_as_bool(resize(target[i],(100,168)))\n",
    "#         asd = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "#         resized_1 = torch.cat((resized_1, asd), dim=0)\n",
    "\n",
    "# target = resized_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IOU(path, labels_dict, pic_size): #model, picsie\n",
    "    IOU = 0\n",
    "    N = 1\n",
    "    for root, dirs, files in os.walk(path, topdown=True):\n",
    "        for name in files:\n",
    "            current_file = str(os.path.join(root, name))\n",
    "            if (current_file[-1] == 'm') and (current_file[current_file.rfind('/') + 1:] in list(labels_dict.keys())):\n",
    "#                 prediction = get_prediction_for_single_pic(current_file, model, pic_size = pic_size)\n",
    "#                 prediction = torch.squeeze(prediction, dim=0)  # [44, h, w]\n",
    "#                 prediction_ = torch.nn.functional.softmax(prediction, dim=0)\n",
    "#                 #                 prediction_1 = torch.max(prediction_[:-1], dim = 0, keepdim=True)\n",
    "                prediction_ = torch.zeros((44,512,870))\n",
    "                name = current_file[current_file.rfind('/') + 1:]\n",
    "                ###############################\n",
    "                empty_labels = torch.zeros(([43, 800, 1360]))\n",
    "                label_bkg = torch.ones(([1, 800, 1360]))\n",
    "                for patch in labels_dict[name]:\n",
    "                    if int(patch[4]) != 43:\n",
    "                        empty_labels[int(patch[4]), int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 1\n",
    "                        label_bkg[0, int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 0\n",
    "                target = torch.cat((empty_labels, label_bkg), dim=0)\n",
    "                for i in range(target.shape[0]):\n",
    "                    if i == 0:\n",
    "                        resized = img_as_bool(resize(target[i], (prediction_.shape[1], prediction_.shape[2])))\n",
    "                        resized_1 = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "                        # 1 25 42\n",
    "                    else:\n",
    "                        resized = img_as_bool(resize(target[i], (prediction_.shape[1], prediction_.shape[2])))\n",
    "                        asd = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "                        resized_1 = torch.cat((resized_1, asd), dim=0)\n",
    "\n",
    "                target = resized_1  # target[13, 42:47, 139:144]\n",
    "                to_show = torch.sum(target[:-1], dim = 0)\n",
    "                print('toshow',to_show.shape)\n",
    "                plt.imshow(to_show)\n",
    "\n",
    "                ###############################\n",
    "                traffic_signs_in_pic = []\n",
    "                for box_coords in labels_dict[name]:\n",
    "                    traffic_signs_in_pic.append(box_coords[-1])\n",
    "                    if pic_size == 800:\n",
    "                        new_coords1 = [int(el) // 8 for el in box_coords[:-1]]\n",
    "                        new_coords1[1] = new_coords1[1] - 2\n",
    "                        new_coords1[3] = new_coords1[3] + 2\n",
    "                        new_coords1[0] = new_coords1[0] - 2\n",
    "                        new_coords1[2] = new_coords1[2] + 2\n",
    "                    elif pic_size == 512:\n",
    "\n",
    "                        new_coords1 = [int(el) for el in box_coords[:-1]]\n",
    "                        new_coords1[1] = (new_coords1[1]*870)//1360 # to be checked\n",
    "                        new_coords1[3] = (new_coords1[3]*870)//1360\n",
    "                        new_coords1[0] = (new_coords1[0]*512)//800\n",
    "                        new_coords1[2] = (new_coords1[2]*512)//800\n",
    "                \n",
    "                    print(f'Row-wise: from {new_coords1[1]} to {new_coords1[3]}, Col-wise: from {new_coords1[0]} to {new_coords1[2]}')\n",
    "\n",
    "                    area_target = torch.sum(target[int(box_coords[-1]), new_coords1[1]:new_coords1[3],\n",
    "                                            new_coords1[0]:new_coords1[2]]).item()\n",
    "                    print(area_target)\n",
    "\n",
    "                    our_value = torch.sum(torch.round(prediction_[int(box_coords[-1]),new_coords1[1]:new_coords1[3],\n",
    "                                                      new_coords1[0]:new_coords1[2]])).item()\n",
    "\n",
    "\n",
    "#                     try:\n",
    "#                         IOU += (our_value / area_target)\n",
    "#                         print('Processing pic: ', name,' ',N,'/1213 : ', IOU/N)\n",
    "#                         N += 1\n",
    "#                     except:\n",
    "#                         print(\"Problems with picture: \", name)\n",
    "#                         to_show = torch.sum(target[:-1], dim=0) # 43 h, w\n",
    "                    plt.show(to_show[\n",
    "                        new_coords1[1]:new_coords1[3],   new_coords1[0]:new_coords1[2]\n",
    "                    ])\n",
    "                    plt.show()\n",
    "\n",
    "    return IOU / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/marconobile/Desktop/CVPR_PRJ/FullIJCNN2013/'\n",
    "labels_dict =  get_labels_dict()\n",
    "\n",
    "# get_IOU(path,labels_dict,pic_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "342\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "1368\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "196\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "380\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "182\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "4140\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "1599\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "3190\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "2116\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "210\n",
      "torch.Size([512, 870]) torch.Size([512, 870])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0]])\n",
      "700\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-a890ce4dfc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mlabels_dict\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mget_labels_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mget_total_IOU_precision_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpic_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-125-a890ce4dfc9e>\u001b[0m in \u001b[0;36mget_total_IOU_precision_recall\u001b[0;34m(path, labels_dict, pic_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mresized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_as_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0masd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         \u001b[0mresized_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresized_1\u001b[0m  \u001b[0;31m# target[13, 42:47, 139:144]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_total_IOU_precision_recall(path, labels_dict, pic_size): #model, picsie\n",
    "    IOU = 0\n",
    "    N = 1\n",
    "    for root, dirs, files in os.walk(path, topdown=True):\n",
    "        for name in files:\n",
    "            current_file = str(os.path.join(root, name))\n",
    "            if (current_file[-1] == 'm') and (current_file[current_file.rfind('/') + 1:] in list(labels_dict.keys())):\n",
    "#                 prediction = get_prediction_for_single_pic(current_file, model, pic_size = pic_size)\n",
    "#                 prediction = torch.squeeze(prediction, dim=0)  # [44, h, w]\n",
    "#                 prediction_ = torch.nn.functional.softmax(prediction, dim=0)\n",
    "#                 #                 prediction_1 = torch.max(prediction_[:-1], dim = 0, keepdim=True)\n",
    "                prediction_ = torch.ones((44,512,870))\n",
    "                name = current_file[current_file.rfind('/') + 1:]\n",
    "        \n",
    "                ###############################\n",
    "                empty_labels = torch.zeros(([43, 800, 1360]))\n",
    "                label_bkg = torch.ones(([1, 800, 1360]))\n",
    "                for patch in labels_dict[name]:\n",
    "                    if int(patch[4]) != 43:\n",
    "                        empty_labels[int(patch[4]), int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 1\n",
    "                        label_bkg[0, int(patch[1]): int(patch[3]), int(patch[0]):int(patch[2])] = 0\n",
    "                target = torch.cat((empty_labels, label_bkg), dim=0)\n",
    "                for i in range(target.shape[0]):\n",
    "                    if i == 0:\n",
    "                        resized = img_as_bool(resize(target[i], (prediction_.shape[1], prediction_.shape[2])))\n",
    "                        resized_1 = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "                        # 1 25 42\n",
    "                    else:\n",
    "                        resized = img_as_bool(resize(target[i], (prediction_.shape[1], prediction_.shape[2])))\n",
    "                        asd = torch.unsqueeze(torch.tensor(resized), dim=0)\n",
    "                        resized_1 = torch.cat((resized_1, asd), dim=0)\n",
    "\n",
    "                target = resized_1  # target[13, 42:47, 139:144]\n",
    "\n",
    "                new_target = torch.sum(target[:-1], dim = 0)\n",
    "                new_prediction = torch.sum(prediction_[:-1], dim = 0)\n",
    "                print(new_target.shape,new_prediction.shape )\n",
    "                \n",
    "#                 print('toshow',to_show.shape)\n",
    "#                 plt.imshow(to_show)\n",
    "\n",
    "                ###############################\n",
    "                traffic_signs_in_pic = []\n",
    "                for box_coords in labels_dict[name]:\n",
    "                    traffic_signs_in_pic.append(box_coords[-1])\n",
    "                    if pic_size == 800:\n",
    "                        new_coords1 = [int(el) // 8 for el in box_coords[:-1]]\n",
    "                        new_coords1[1] = new_coords1[1] - 2\n",
    "                        new_coords1[3] = new_coords1[3] + 2\n",
    "                        new_coords1[0] = new_coords1[0] - 2\n",
    "                        new_coords1[2] = new_coords1[2] + 2\n",
    "                    elif pic_size == 512:\n",
    "\n",
    "                        new_coords1 = [int(el) for el in box_coords[:-1]]\n",
    "                        new_coords1[1] = (new_coords1[1]*870)//1360 # to be checked\n",
    "                        new_coords1[3] = (new_coords1[3]*870)//1360\n",
    "                        new_coords1[0] = (new_coords1[0]*512)//800\n",
    "                        new_coords1[2] = (new_coords1[2]*512)//800\n",
    "                        \n",
    "                        \n",
    "                    print(new_target[new_coords1[1]+1:new_coords1[3]+2,\n",
    "                                            new_coords1[0]-1:new_coords1[2]+1])\n",
    "                    \n",
    "                \n",
    "#                     print(f'Row-wise: from {new_coords1[1]} to {new_coords1[3]}, Col-wise: from {new_coords1[0]} to {new_coords1[2]}')\n",
    "\n",
    "                    area_target = torch.sum(new_target[new_coords1[1]:new_coords1[3],\n",
    "                                            new_coords1[0]:new_coords1[2]]).item()\n",
    "                    print(area_target)\n",
    "\n",
    "#                     our_value = torch.sum(torch.round(new_prediction[new_coords1[1]:new_coords1[3],\n",
    "#                                                       new_coords1[0]:new_coords1[2]])).item()\n",
    "\n",
    "\n",
    "#                     try:\n",
    "#                         IOU += (our_value / area_target)\n",
    "#                         print('Processing pic: ', name,' ',N,'/1213 : IOU = ',our_value,'/',area_target,' = ', IOU/N)\n",
    "#                         N += 1\n",
    "#                     except:\n",
    "#                         print(\"Problems with picture: \", name)\n",
    "#                         to_show = torch.sum(target[:-1], dim=0) # 43 h, w\n",
    "\n",
    "    return IOU / N\n",
    "\n",
    "path = '/Users/marconobile/Desktop/CVPR_PRJ/FullIJCNN2013/'\n",
    "labels_dict =  get_labels_dict()\n",
    "\n",
    "get_total_IOU_precision_recall(path,labels_dict,pic_size = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('DL_pytorch': conda)",
   "language": "python",
   "name": "python37564bitdlpytorchconda4df763b7fd43405fa82f124b7d8aa0d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
